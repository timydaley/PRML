{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Consider the sum of squares error function given by \n",
    "$$\n",
    "\\mathrm{E} (\\boldsymbol{w}) = \\frac{1}{2} \\sum_{n = 1}^{N} \\big( y(x_{n}, \\boldsymbol{w}) - t_{n} \\big)^{2}\n",
    "$$\n",
    "in which the function $y(x, \\boldsymbol{w})$ is given by the polynomial $\\sum_{j = 0}^{M} w_{j} x^{j}$.  Show that the coefficients $\\boldsymbol{w} = \\{ w_{i} \\}$ that minimize this error function are given by the solution to the following set of linear equations\n",
    "$$\n",
    "\\sum_{j=0}^{M} A_{ij} w_{j} = T_{i},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "A_{ij}= \\sum_{n = 1}^{N} x_{n}^{i+j} \\quad \\text{ and } \\quad T_{i} = \\sum_{n = 1}^{N} x_{n}^{i} t_{n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: \n",
    "Substitue $y(x, \\boldsymbol{w})$ into $\\mathrm{E}(w)$ and then take derivatives with respect to $w_{j}$ to obtain $\\frac{\\partial}{\\partial w_{i}} \\mathrm{E} (w) = \\sum_{n = 1}^{N} \\big(\\sum_{j = 0}^{M} w_{j} x_{n}^{j} - t_{n}) x_{n}^{i} = 0$.  Therefore $\\sum_{n = 1}^{N} x_{n}^{i} \\sum_{j = 0}^{M} w_{j} x_{n}^{j} = \\sum_{n = 1}^{N} x_{n}^{i} t_{n}$, and the result follows.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.2 Write down the set of coupled linear equations, analogous to (1.122), satisfied by the coefficients $w_{i}$ that minimize the regularized sum-of-squares error function given by $\\tilde{\\mathrm{E}} (w) = \\frac{1}{2} \\sum_{n = 1}^{N} \\big( y(x_{n}, w) - t_{n} \\big)^{2} + \\frac{\\lambda}{2} || w ||^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: \n",
    "Note first that $|| w ||^{2} = \\sum_{j = 0}^{M} w_{j}^{2}$.  Taking derivatives of the penalized error function gives $\\frac{\\partial}{\\partial w_{i}} \\tilde{\\mathrm{E}} (w) = \\sum_{n = 1}^{N} \\big( \\sum_{j = 0}^{M} w_{j} x_{n}^{j} - t_{n} \\big) x_{n}^{i} - \\lambda w_{i} = 0$, and therefore $\\sum_{n = 1}^{N} t_{n} x_{n}^{i} = \\sum_{n = 1}^{N} x_{n}^{i} \\sum_{j = 0}^{M} w_{j} x_{n}^{j} - \\lambda w_{i}$.  If we let $T_{i}$ and $A_{ij}$ be defined as above, then we get $\\sum_{j = 0}^{M} A_{ij} w_{j} - \\lambda w_{i} = T_{i}$ for all $i$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3  Suppose that we have 3 colored boxes: read, blue, and green.  The red box ($r$) contains 3 apples, 4 oranges, and 3 limes; the blue box ($b$) contains 1 apple and 1 oranges; and the green box ($g$) contains 3 apples, 3 oranges, and 4 limes.  If a box is chosen at random with $\\Pr(r) = 0.2$, $\\Pr(b) = 0.2$, and $\\Pr(g) = 0.6$, and we remove one piece of fruit from the box, then what is the probability of selecting an apple.  If we observe that the selected fruit is an orange, what is the probability that we selected from the green box?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:  \n",
    "$\\Pr(\\text{apple}) = \\Pr(\\text{apple} | r) \\Pr(r) + \\Pr(\\text{apple} | b) \\Pr(b) + \\Pr(\\text{apple} | g) \\Pr(g) = 0.3 \\cdot 0.2 + 0.5 \\cdot 0.2 + 0.3 \\cdot 0.6 = 0.34$. \n",
    "$\\Pr(g | \\text{orange}) = \\Pr(\\text{orange}, g) / \\Pr(\\text{orange})$.  $\\Pr(\\text{orange}) = 0.4 \\cdot 0.2 + 0.5 \\cdot 0.2 + 0.3 \\cdot 0.6 = 0.36$ and $\\Pr(\\text{orange}, g) = 0.3 \\cdot 0.6 = 0.18$, therefore $\\Pr(g | \\text{orange}) = 0.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4  Consider a probability density $p_{x}(x)$ defined over a continuous variable $x$ and suppose that we make a non-linear change of variables using $x = g(y)$, so that the density transforms to $p_{y}(y) = p_{x}\\big( g(y) \\big) | g^{\\prime}(y)|$.  By differentiating the preceding equation, show that the location $\\hat{y}$ of the maximum of the density in $y$ is not in general related to the location $\\hat{x}$ of the maximum of the density of $x$ by the simple functional relation $\\hat{x} = g(\\hat{y})$ as a consequence of the Jacobian factor.  This shows that the maximum of a probability density depends on the choice of variable.  Verify that in the case of the a linear transformation, the location of the maximum transforms in the same way as the variable itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: \n",
    "If $g(y) = a x$ for some $a$, then $| g^{\\prime} (y)| = |a|$ and therefore will not affect the location of the maximum (up to the transformation).  On the other hand, $\\frac{\\partial}{\\partial y} p_{y}(y) = p^{\\prime}_{x} \\big( g(y) \\big) g^{\\prime}(y) | g^{\\prime}(y) |$, so that the terms $g^{\\prime}(y) | g^{\\prime}(y) |$ will affect where the maximum occurs.  Say if $g(y)$ is a strongly concave function with a well defined maximum and $p_{x}$ is relatively broad.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Using definition (1.38), show that $\\text{var}\\big(f(x) \\big)$ satisfies (1.39)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "(1.38) says that $\\text{var} \\big( f(x) \\big) = \\mathrm{E} \\bigg( \\big( f(x) - \\mathrm{E} (f(x)) \\big)^{2} \\bigg)$.  Expanding this out gives $\\text{var} \\big( f(x) \\big) = \\mathrm{E} \\bigg( f(x) \\cdot f(x) - 2 f(x) \\mathrm{E} \\big(f(x) \\big) + \\big( \\mathrm{E}(f(x)) \\big)^{2} \\bigg)$.  Note that expectation is linear and therefore $\\text{var} \\big( f(x) \\big) = \\mathrm{E} \\bigg( f(x)^{2} \\bigg) - 2 \\mathrm{E} \\big(f(x) \\big) \\cdot \\mathrm{E} \\big(f(x) \\big)  + \\bigg( \\mathrm{E} \\big(f(x) \\big)  \\bigg)^{2}$, which of course results in (1.39)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 Show that if two variables $x$ and $y$ are independent then their covariance is equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "$\\text{cov}(x, y) = \\mathrm{E} \\Big( \\big( x - \\mathrm{E} (x) \\big) \\big( y - \\mathrm{E} (y) \\big) \\Big) = \\mathrm{E} \\big( xy - x \\mathrm{E} (y) - y \\mathrm{E} (x) + \\mathrm{E} (x) \\mathrm{E} (y) \\big)$.  By the linearity of expectation this is equal to $\\mathrm{E} (xy) - \\mathrm{E} (x) \\mathrm{E} (y)$.  Since $x$ and $y$ are independent, $\\mathrm{E}(xy) = \\mathrm{E} (x) \\mathrm{E} (y)$, and the covariance is equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7  In this exercise we prove the normalization condition (1.48) for the univariate Gaussian.  To do this consider the intergral $I = \\int_{- \\infty}^{\\infty} \\text{exp} \\big( - \\frac{1}{2 \\sigma^{2}} x^{2} \\big) dx$, which we can evaluate by first writing its square in the form $I^{2} = \\int_{- \\infty}^{\\infty} \\int_{- \\infty}^{\\infty} \\text{exp} \\big( - \\frac{1}{2 \\sigma^{2}} x^{2} - - \\frac{1}{2 \\sigma^{2}} y^{2} \\big) dx dy$.  Now make the transformation from Cartesian coordinates to polar coordinates and then substitute $u = r^{2}$.  Show that by performing the integrals over $\\theta$, then $u$, and then taking the square root of both sides "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
